{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "247291b3-da31-4eef-a89e-0360ec300bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import resspect\n",
    "import pandas as pd\n",
    "from resspect import request_TOM_data\n",
    "from resspect import fit_TOM, fit\n",
    "from resspect import submit_queries_to_TOM\n",
    "from resspect import time_domain_loop\n",
    "from resspect.tom_client import TomClient\n",
    "from resspect import time_domain_loop\n",
    "from resspect import TimeDomainConfiguration\n",
    "import os\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30f96d54-c60e-4ce5-b150-eb6b57c50e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###ðŸ”² Need to import this from RESSPECT\n",
    "###ðŸ”² Need to put this updated version in RESSPECT \n",
    "def update_pool_stash(day: int):\n",
    "    #ðŸ”²check if a directory exists to contain features and if it doesn't, make it!\n",
    "    outdir = 'TOM_days_storage'\n",
    "\n",
    "    #should we store old features somewhere? makes it easier to add training objs\n",
    "    #would want to add current MJD, maybe first MJD, and peak MJD\n",
    "    if day!=0:\n",
    "        current_stash_path = outdir+'/TOM_compiled_features_day'+str(day-1)+'.csv'\n",
    "    elif day==0:\n",
    "        current_stash_path = outdir+'/TOM_hot_features_day_'+str(day)+'.csv'\n",
    "    \n",
    "    new_night_path = outdir+'/TOM_hot_features_day_'+str(day)+'.csv'\n",
    "    \n",
    "    #read in current stash as list of strings\n",
    "    with open(current_stash_path, 'r') as f:\n",
    "        current_stash = f.readlines()\n",
    "    #read in new night as list of strings\n",
    "    with open(new_night_path, 'r') as f:\n",
    "        new_night = f.readlines()\n",
    "\n",
    "    curent_stash_df = pd.read_csv(current_stash_path)\n",
    "    new_night_df = pd.read_csv(new_night_path)\n",
    "\n",
    "    compiled_df = pd.concat([curent_stash_df,new_night_df]).drop_duplicates('id', keep='last')\n",
    "    compiled_list = compiled_df.to_string(index=False).split('\\n')\n",
    "    compiled_comsep_list = [','.join(ele.split()) for ele in compiled_list]\n",
    "    return_string = '\\n'.join(compiled_comsep_list)\n",
    "\n",
    "    output_path = outdir+'/TOM_compiled_features_day'+str(day)+'.csv'\n",
    "    # rewrite the file \n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(return_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01223312-9b79-4355-817b-fe98497c23de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ðŸ”² Need to remove no longer hot SN, will help speed things up\n",
    "#ðŸ”² in the mean time remove after 15 days\n",
    "def remove_from_pool_stash(day):\n",
    "    #ðŸ”²check if a directory exists to contain features and if it doesn't, make it!\n",
    "    outdir = 'TOM_days_storage'\n",
    "    current_stash_path = outdir+'/TOM_hot_features_day_'+str(day)+'.csv'\n",
    "    \n",
    "    #read in current stash as list of strings\n",
    "    with open(current_stash_path, 'r') as f:\n",
    "        current_stash = f.readlines()\n",
    "    \n",
    "    curent_stash_df = pd.read_csv(current_stash_path)\n",
    "\n",
    "\n",
    "    #ðŸ”² Need to remove old obj\n",
    "    remove_old_obj_df = current_stash_df[current_stash_df[\"date_added\"] > day-15]\n",
    "\n",
    "    removed_list = remove_old_obj_df.to_string(index=False).split('\\n')\n",
    "    removed_comsep_list = [','.join(ele.split()) for ele in removed_list]\n",
    "    return_string = '\\n'.join(removed_comsep_list)\n",
    "\n",
    "    output_path = outdir+'/TOM_compiled_features_day'+str(day)+'.csv'\n",
    "    #gotta rewrite the file dummy\n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(return_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c24acd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_info = [\n",
    "        'hostgal_snsep',\n",
    "        'hostgal_ellipticity',\n",
    "        'hostgal_sqradius',\n",
    "        'hostgal_mag_u',\n",
    "        'hostgal_mag_g',\n",
    "        'hostgal_mag_r',\n",
    "        'hostgal_mag_i',\n",
    "        'hostgal_mag_z',\n",
    "        'hostgal_mag_y',\n",
    "        'hostgal_magerr_u',\n",
    "        'hostgal_magerr_g',\n",
    "        'hostgal_magerr_r',\n",
    "        'hostgal_magerr_i',\n",
    "        'hostgal_magerr_z',\n",
    "        'hostgal_magerr_y',\n",
    "    ]\n",
    "\n",
    "from laiss_resspect_classifier.elasticc2_laiss_feature_extractor import Elasticc2LaissFeatureExtractor\n",
    "\n",
    "def validate_objects(objects_to_test):\n",
    "    fe = Elasticc2LaissFeatureExtractor()\n",
    "    good_objs = []\n",
    "\n",
    "    for t_obj in objects_to_test:\n",
    "        fe.photometry= pd.DataFrame(t_obj['photometry'])\n",
    "        fe.id = t_obj['objectid']\n",
    "\n",
    "        fe.additional_info = {}\n",
    "        for info in additional_info:\n",
    "            fe.additional_info[info] = t_obj[info]\n",
    "\n",
    "        res = fe.fit_all()\n",
    "        if res:\n",
    "            good_objs.append(t_obj)\n",
    "\n",
    "    return good_objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd30be07-410d-44f3-a878-dfed0be1fafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phot(obj_df):\n",
    "\n",
    "    tom = TomClient(url = \"https://desc-tom-2.lbl.gov\", username = 'awoldag', passwordfile = '../../../password.txt')\n",
    "\n",
    "    # get all of the photometry at once\n",
    "    ids = obj_df['diaobject_id'].tolist()\n",
    "    res = tom.post('db/runsqlquery/',\n",
    "                          json={ 'query': 'SELECT diaobject_id, filtername, midpointtai, psflux, psfluxerr'  \n",
    "                                ' FROM elasticc2_ppdbdiaforcedsource' \n",
    "                              ' WHERE diaobject_id IN (%s) ORDER BY diaobject_id, filtername, midpointtai;' % (', '.join(str(id) for id in ids)),\n",
    "                                'subdict': {} } )\n",
    "    all_phot = res.json()['rows']\n",
    "    all_phot_df = pd.DataFrame(all_phot)\n",
    "    # if you need mag from the arbitrary flux-\n",
    "    all_phot_df['mag'] = -2.5*np.log10(all_phot_df['psflux']) + 27.5\n",
    "    all_phot_df['magerr'] = 2.5/np.log(10) * all_phot_df['psfluxerr']/all_phot_df['psflux']\n",
    "\n",
    "    #! Need to send Rob a message to ask that these features be included when querying for hot super nova\n",
    "    host_res = tom.post('db/runsqlquery/',\n",
    "                          json={ 'query': 'SELECT diaobject_id, hostgal_mag_u, hostgal_mag_g, hostgal_mag_r, hostgal_mag_i, hostgal_mag_z, hostgal_mag_Y, hostgal_magerr_u, hostgal_magerr_g, hostgal_magerr_r, hostgal_magerr_i, hostgal_magerr_z, hostgal_magerr_Y, hostgal_snsep, hostgal_ellipticity, hostgal_sqradius'\n",
    "                                ' FROM elasticc2_ppdbdiaobject'\n",
    "                              ' WHERE diaobject_id IN (%s) ORDER BY diaobject_id;' % (', '.join(str(id) for id in ids)),\n",
    "                                'subdict': {} } )\n",
    "    all_host = host_res.json()['rows']\n",
    "\n",
    "\n",
    "    # format into a list of dicts\n",
    "    data = []\n",
    "    for idx, obj in obj_df.iterrows():\n",
    "        phot = all_phot_df[all_phot_df['diaobject_id'] == obj['diaobject_id']]\n",
    "\n",
    "        phot_d = {}\n",
    "        phot_d['objectid'] = int(obj['diaobject_id'])\n",
    "        phot_d['sncode'] = int(obj['gentype'])\n",
    "        phot_d['redshift'] = obj['zcmb']\n",
    "        phot_d['ra'] = obj['ra']\n",
    "        phot_d['dec'] = obj['dec']\n",
    "        phot_d['photometry'] = phot[['filtername', 'midpointtai', 'psflux', 'psfluxerr', 'mag', 'magerr']].to_dict(orient='list')\n",
    "\n",
    "        phot_d['photometry']['band'] = phot_d['photometry']['filtername']\n",
    "        phot_d['photometry']['mjd'] = phot_d['photometry']['midpointtai']\n",
    "        phot_d['photometry']['flux'] = phot_d['photometry']['psflux']\n",
    "        phot_d['photometry']['fluxerr'] = phot_d['photometry']['psfluxerr']\n",
    "        phot_d['photometry']['mag'] = phot_d['photometry']['mag']\n",
    "        phot_d['photometry']['magerr'] = phot_d['photometry']['magerr']\n",
    "        del phot_d['photometry']['filtername']\n",
    "        del phot_d['photometry']['midpointtai']\n",
    "        del phot_d['photometry']['psflux']\n",
    "        del phot_d['photometry']['psfluxerr']\n",
    "        phot_d = {**phot_d, **all_host[idx]}\n",
    "        del phot_d['diaobject_id']\n",
    "        data.append(phot_d)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da5d212-81e7-43d8-8592-f9e8ccae00fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phot_orig(obj_df):\n",
    "    # get all of the photometry at once\n",
    "    ids = obj_df['diaobject_id'].tolist()\n",
    "    res = tom.post('db/runsqlquery/',\n",
    "                          json={ 'query': 'SELECT diaobject_id, filtername, midpointtai, psflux, psfluxerr'  \n",
    "                                ' FROM elasticc2_ppdbdiaforcedsource' \n",
    "                              ' WHERE diaobject_id IN (%s) ORDER BY diaobject_id, filtername, midpointtai;' % (', '.join(str(id) for id in ids)),\n",
    "                                'subdict': {} } )\n",
    "    all_phot = res.json()['rows']\n",
    "    all_phot_df = pd.DataFrame(all_phot)\n",
    "    # if you need mag from the arbitrary flux\n",
    "    # all_phot_df['mag'] = -2.5*np.log10(all_phot_df['psflux']) + 27.5\n",
    "    # all_phot_df['magerr'] = 2.5/np.log(10) * all_phot_df['psfluxerr']/all_phot_df['psflux']\n",
    "\n",
    "    # format into a list of dicts\n",
    "    data = []\n",
    "    for idx, obj in obj_df.iterrows():\n",
    "        phot = all_phot_df[all_phot_df['diaobject_id'] == obj['diaobject_id']]\n",
    "        \n",
    "        phot_d = {}\n",
    "        phot_d['objectid'] = int(obj['diaobject_id'])\n",
    "        phot_d['sncode'] = int(obj['gentype'])\n",
    "        phot_d['redshift'] = obj['zcmb']\n",
    "        phot_d['ra'] = obj['ra']\n",
    "        phot_d['dec'] = obj['dec']\n",
    "        phot_d['photometry'] = phot[['filtername', 'midpointtai', 'psflux', 'psfluxerr']].to_dict(orient='list')\n",
    "\n",
    "        phot_d['photometry']['band'] = phot_d['photometry']['filtername']\n",
    "        phot_d['photometry']['mjd'] = phot_d['photometry']['midpointtai']\n",
    "        phot_d['photometry']['flux'] = phot_d['photometry']['psflux']\n",
    "        phot_d['photometry']['fluxerr'] = phot_d['photometry']['psfluxerr']\n",
    "        del phot_d['photometry']['filtername']\n",
    "        del phot_d['photometry']['midpointtai']\n",
    "        del phot_d['photometry']['psflux']\n",
    "        del phot_d['photometry']['psfluxerr']\n",
    "        \n",
    "        data.append(phot_d)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae4fd9b-6866-40ce-a536-8c50c2337c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAKE INITIAL TRAINING SET \n",
    "objs = []\n",
    "\n",
    "tom = TomClient(url = \"https://desc-tom-2.lbl.gov\", username = 'awoldag', passwordfile = '../../../password.txt')\n",
    "\n",
    "res = tom.post('db/runsqlquery/',\n",
    "                        json={ 'query': 'SELECT diaobject_id, gentype, zcmb, peakmjd,' \n",
    "                              ' peakmag_g, ra, dec FROM elasticc2_diaobjecttruth WHERE peakmjd>61300 and peakmjd<61309 and gentype=10 limit 10;', \n",
    "                             'subdict': {}} )\n",
    "objs.extend(res.json()['rows'])\n",
    "\n",
    "res = tom.post('db/runsqlquery/',\n",
    "                        json={ 'query': 'SELECT diaobject_id, gentype, zcmb, peakmjd,' \n",
    "                              ' peakmag_g, ra, dec FROM elasticc2_diaobjecttruth WHERE peakmjd>61300 and peakmjd<61309 and gentype=21 limit 5;', \n",
    "                             'subdict': {}} )\n",
    "objs.extend(res.json()['rows'])\n",
    "\n",
    "res = tom.post('db/runsqlquery/',\n",
    "                        json={ 'query': 'SELECT diaobject_id, gentype, zcmb, peakmjd,' \n",
    "                              ' peakmag_g, ra, dec FROM elasticc2_diaobjecttruth WHERE peakmjd>61300 and peakmjd<61309 and gentype=31 limit 5;', \n",
    "                             'subdict': {}} )\n",
    "objs.extend(res.json()['rows'])\n",
    "\n",
    "training_objs = get_phot(pd.DataFrame(objs))\n",
    "good_objs = validate_objects(training_objs)\n",
    "\n",
    "outdir = 'TOM_days_storage'\n",
    "\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "\n",
    "# ðŸ”² change this to fit()\n",
    "feature_extraction_method = 'laiss_resspect_classifier.elasticc2_laiss_feature_extractor.Elasticc2LaissFeatureExtractor'\n",
    "fit(\n",
    "    good_objs,\n",
    "    output_features_file = outdir+'/TOM_training_features',\n",
    "    feature_extractor = feature_extraction_method,\n",
    "    filters='ZTF',\n",
    "    additional_info=additional_info,\n",
    ")\n",
    "data = pd.read_csv('TOM_days_storage/TOM_training_features',index_col=False)\n",
    "data['orig_sample'] = 'train'\n",
    "data[\"type\"] = np.where(data[\"sncode\"] == 10, 'Ia', 'other')\n",
    "data.to_csv('TOM_days_storage/TOM_training_features',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52da9afa-5b1f-4c66-80b6-77608dcd30ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAKE TEST SET \n",
    "objs = []\n",
    "\n",
    "tom = TomClient(url = \"https://desc-tom-2.lbl.gov\", username = 'awoldag', passwordfile = '../../../password.txt')\n",
    "\n",
    "res = tom.post('db/runsqlquery/',\n",
    "                        json={ 'query': 'SELECT diaobject_id, gentype, zcmb, peakmjd,' \n",
    "                              ' peakmag_g, ra, dec FROM elasticc2_diaobjecttruth WHERE peakmjd>61310 and peakmjd<61339 and gentype=10 limit 1000;', \n",
    "                             'subdict': {}} )\n",
    "objs.extend(res.json()['rows'])\n",
    "\n",
    "res = tom.post('db/runsqlquery/',\n",
    "                        json={ 'query': 'SELECT diaobject_id, gentype, zcmb, peakmjd,' \n",
    "                              ' peakmag_g, ra, dec FROM elasticc2_diaobjecttruth WHERE peakmjd>61310 and peakmjd<61339 and gentype=21 limit 500;', \n",
    "                             'subdict': {}} )\n",
    "objs.extend(res.json()['rows'])\n",
    "\n",
    "res = tom.post('db/runsqlquery/',\n",
    "                        json={ 'query': 'SELECT diaobject_id, gentype, zcmb, peakmjd,' \n",
    "                              ' peakmag_g, ra, dec FROM elasticc2_diaobjecttruth WHERE peakmjd>61310 and peakmjd<61339 and gentype=31 limit 500;', \n",
    "                             'subdict': {}} )\n",
    "objs.extend(res.json()['rows'])\n",
    "\n",
    "test_objs = get_phot(pd.DataFrame(objs))\n",
    "good_objs = validate_objects(test_objs)\n",
    "\n",
    "outdir = 'TOM_days_storage'\n",
    "\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "\n",
    "# ðŸ”² change this to fit()\n",
    "feature_extraction_method = 'laiss_resspect_classifier.elasticc2_laiss_feature_extractor.Elasticc2LaissFeatureExtractor'\n",
    "fit(\n",
    "    good_objs,\n",
    "    output_features_file = outdir+'/TOM_testing_features',\n",
    "    feature_extractor = feature_extraction_method,\n",
    "    filters='ZTF',\n",
    "    additional_info=additional_info,\n",
    ")\n",
    "data = pd.read_csv('TOM_days_storage/TOM_testing_features',index_col=False)\n",
    "data['orig_sample'] = 'train'\n",
    "data[\"type\"] = np.where(data[\"sncode\"] == 10, 'Ia', 'other')\n",
    "data.to_csv('TOM_days_storage/TOM_testing_features',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4ff59b-e018-4777-a709-736b98fb8e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#âœ… MAKE VALIDATION SET \n",
    "objs = []\n",
    "\n",
    "tom = TomClient(url = \"https://desc-tom-2.lbl.gov\", username = 'awoldag', passwordfile = '../../../password.txt')\n",
    "\n",
    "res = tom.post('db/runsqlquery/',\n",
    "                        json={ 'query': 'SELECT diaobject_id, gentype, zcmb, peakmjd,' \n",
    "                              ' peakmag_g, ra, dec FROM elasticc2_diaobjecttruth WHERE peakmjd>61340 and gentype=10 limit 1000;', \n",
    "                             'subdict': {}} )\n",
    "objs.extend(res.json()['rows'])\n",
    "\n",
    "res = tom.post('db/runsqlquery/',\n",
    "                        json={ 'query': 'SELECT diaobject_id, gentype, zcmb, peakmjd,' \n",
    "                              ' peakmag_g, ra, dec FROM elasticc2_diaobjecttruth WHERE peakmjd>61340 and gentype=21 limit 500;', \n",
    "                             'subdict': {}} )\n",
    "objs.extend(res.json()['rows'])\n",
    "\n",
    "res = tom.post('db/runsqlquery/',\n",
    "                        json={ 'query': 'SELECT diaobject_id, gentype, zcmb, peakmjd,' \n",
    "                              ' peakmag_g, ra, dec FROM elasticc2_diaobjecttruth WHERE peakmjd>61340 and gentype=31 limit 500;', \n",
    "                             'subdict': {}} )\n",
    "objs.extend(res.json()['rows'])\n",
    "\n",
    "val_objs = get_phot(pd.DataFrame(objs))\n",
    "good_objs = validate_objects(val_objs)\n",
    "\n",
    "outdir = 'TOM_days_storage'\n",
    "\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "\n",
    "# ðŸ”² change this to fit()\n",
    "feature_extraction_method = 'laiss_resspect_classifier.elasticc2_laiss_feature_extractor.Elasticc2LaissFeatureExtractor'\n",
    "fit(\n",
    "    good_objs,\n",
    "    output_features_file = outdir+'/TOM_validation_features',\n",
    "    feature_extractor = feature_extraction_method,\n",
    "    filters='ZTF',\n",
    "    additional_info=additional_info,\n",
    ")\n",
    "data = pd.read_csv('TOM_days_storage/TOM_validation_features',index_col=False)\n",
    "data['orig_sample'] = 'train'\n",
    "data[\"type\"] = np.where(data[\"sncode\"] == 10, 'Ia', 'other')\n",
    "data.to_csv('TOM_days_storage/TOM_validation_features',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcadd173-4778-406c-80e4-4dccdddc8365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_night(day):    \n",
    "                            #ðŸ”² check for new spec+classification for training set before running the loop\n",
    "    #âœ…flag for simulated vs real data\n",
    "                            #ðŸ”²implement an auto-check\n",
    "    #âœ…first thing, check if it's real or sim data\n",
    "                            #ðŸ”²MAKE CLEAR WHAT IS FOR REAL DATA VS SIM (eg. real data will want date)\n",
    "    #âœ…request light curve data from the TOM - for real and simulated\n",
    "    \n",
    "    #get new lc info from TOM (from yesterday (for now))\n",
    "    data_dic = request_TOM_data(url = \"https://desc-tom-2.lbl.gov\", username='awoldag',\n",
    "                                passwordfile='../../../password.txt', detected_in_last_days = 1,\n",
    "                                mjdnow = 60796+day-1, cheat_gentypes = [82, 10, 21, 27, 26, 37, 32, 36, 31, 89])\n",
    "    data_dic=data_dic['diaobject']\n",
    "\n",
    "    # feature_extraction_method = 'Malanchev'\n",
    "    feature_extraction_method = 'laiss_resspect_classifier.elasticc2_laiss_feature_extractor.Elasticc2LaissFeatureExtractor'\n",
    "    # classifier = 'RandomForest'\n",
    "    classifier = 'laiss_resspect_classifier.laiss_classifier.LaissRandomForest'\n",
    "\n",
    "    #âœ…run that data through RESSPECT to get features\n",
    "                            #ðŸ”²at some point, cut out objects that are not likely SN - do this before it gets to RESSPECT probably\n",
    "    #âœ…clarify out file argument/data base\n",
    "\n",
    "\n",
    "    #    if day >= 0:\n",
    "#        file_name = outdir+'/TOM_hot_features_day_'+str(day)+'.csv'\n",
    "#    else:\n",
    "#        file_name = outdir+'/TOM_hot_features.csv'\n",
    "    \n",
    "    #get features from that data\n",
    "    outdir = 'TOM_days_storage'\n",
    "    file_name = outdir+'/TOM_hot_features_day_'+str(day-1)+'.csv'\n",
    "        \n",
    "    fit(\n",
    "        data_dic,\n",
    "        output_features_file = file_name,\n",
    "        feature_extractor = feature_extraction_method,\n",
    "        filters='ZTF',\n",
    "        additional_info=additional_info\n",
    "    )\n",
    "    data = pd.read_csv(file_name, index_col=False)\n",
    "    data['orig_sample'] = 'pool'\n",
    "    #add date added so that we can remove when they are too old\n",
    "\n",
    "    \n",
    "    data.to_csv(file_name,index=False)\n",
    "\n",
    "    # -------------------------\n",
    "\n",
    "    #get new lc info from TOM (for today)\n",
    "    data_dic = request_TOM_data(url = \"https://desc-tom-2.lbl.gov\",username='awoldag',\n",
    "                                passwordfile='../../../password.txt',detected_in_last_days = 1, mjdnow = 60796+day, \n",
    "                                cheat_gentypes = [82, 10, 21, 27, 26, 37, 32, 36, 31, 89])\n",
    "    data_dic=data_dic['diaobject']\n",
    "    \n",
    "    #get features from that data\n",
    "    outdir = 'TOM_days_storage'\n",
    "    file_name = outdir+'/TOM_hot_features_day_'+str(day)+'.csv'\n",
    "        \n",
    "    fit(\n",
    "        data_dic,\n",
    "        output_features_file = file_name,\n",
    "        feature_extractor = feature_extraction_method,\n",
    "        filters='ZTF',\n",
    "        additional_info=additional_info,\n",
    "    )\n",
    "    data = pd.read_csv(file_name, index_col=False)\n",
    "    data['orig_sample'] = 'pool'\n",
    "    \n",
    "    data.to_csv(file_name,index=False)\n",
    "    \n",
    "\n",
    "    #âœ…update feature lists\n",
    "    #âœ… change this so that we have a new file for each day - just make it so that update_pool_stash writes to a new file \n",
    "    #âœ…(and puts this file in a directory)\n",
    "                            #ðŸ”² Probably want to make this more general file names down the line.....\n",
    "#    update_pool_stash(day)\n",
    "\n",
    "#! #############################################\n",
    "#! Moved everything below here to a new function\n",
    "#! #############################################\n",
    "\n",
    "    # # run the loop to get queried objects and updated metrics\n",
    "    # days = [day-1, day+1]                                # first and last day of the survey\n",
    "    # training = None                           # if int take int number of objs\n",
    "    #                                                     # for initial training, 50% being Ia\n",
    "    \n",
    "    # strategy = 'UncSampling'                        # learning strategy\n",
    "    # batch = 5                                       # if int, ignore cost per observation,\n",
    "    #                                                      # if None find optimal batch size\n",
    "    \n",
    "    # sep_files = True                               # if True, expects train, test and\n",
    "    #                                                     # validation samples in separate filess\n",
    "    \n",
    "    # path_to_features_dir = 'TOM_days_storage/'   # folder where the files for each day are stored\n",
    "    \n",
    "    # # output results for metrics\n",
    "    # output_metrics_file = 'results/metrics_' + strategy + '_' + str('ini_train_set') + \\\n",
    "    #                        '_batch' + str(batch) +  '.csv'\n",
    "    \n",
    "    # # output query sample\n",
    "    # output_query_file = 'results/queried_' + strategy + '_' + str('ini_train_set') + \\\n",
    "    #                         '_batch' + str(batch) + '_day_'+ str(day) + '.csv'\n",
    "    \n",
    "    # path_to_ini_files = {}\n",
    "    \n",
    "    # # features from full light curves for initial training sample\n",
    "    # path_to_ini_files['train'] = 'TOM_days_storage/TOM_training_features'\n",
    "    # path_to_ini_files['test'] = 'TOM_days_storage/TOM_testing_features'\n",
    "    # path_to_ini_files['validation'] = 'TOM_days_storage/TOM_validation_features'\n",
    "    \n",
    "    # survey='LSST'\n",
    "    \n",
    "\n",
    "    # n_estimators = 1000                             # number of trees in the forest\n",
    "    \n",
    "\n",
    "    # screen = False                                  # if True will print many things for debuging\n",
    "    # fname_pattern = ['TOM_hot_features_day_', '.csv']                # pattern on filename where different days\n",
    "    #                                                     # are stored\n",
    "    \n",
    "    # queryable= False                                 # if True, check brightness before considering\n",
    "    #                                                     # an object queryable    \n",
    "    \n",
    "    # # run time domain loop\n",
    "    # time_domain_loop(TimeDomainConfiguration(days=days, output_metrics_file=output_metrics_file,\n",
    "    #                  output_queried_file=output_query_file,\n",
    "    #                  path_to_ini_files=path_to_ini_files,\n",
    "    #                  path_to_features_dir=path_to_features_dir,\n",
    "    #                  strategy=strategy, fname_pattern=fname_pattern, batch=batch,\n",
    "    #                  classifier=classifier,\n",
    "    #                  sep_files=sep_files,\n",
    "    #                  survey=survey, queryable=queryable,\n",
    "    #                  feature_extraction_method=feature_extraction_method),\n",
    "    #                  screen=screen, n_estimators=n_estimators,\n",
    "    #                  )\n",
    "\n",
    "    # #ðŸ”² do we want higher entropy in our returned objects?\n",
    "    # # Read in RESSPECT requests to input to TOM format\n",
    "    # ids = list(pd.read_csv(output_query_file)['id'])\n",
    "    # ids = [int(id) for id in ids]\n",
    "    # num = int(len(ids)/5)\n",
    "    # mod = len(ids)%5\n",
    "    # num_list = [num]*5\n",
    "    # mod_list = []\n",
    "    # for i in range(mod):\n",
    "    #     mod_list.append(1)\n",
    "    # rem = 5-len(mod_list)\n",
    "    # mod_list = mod_list+[0]*rem\n",
    "    # num_list=list(np.asarray(num_list)+mod_list)\n",
    "    # priorities = []\n",
    "    # priorities.append([1]*num_list[0]+[2]*num_list[1]+[3]*num_list[2]+[4]*num_list[3]+[5]*num_list[4])    \n",
    "    # priorities = priorities[0]\n",
    "    \n",
    "    # # send these queried objects to the TOM\n",
    "    # # submit_queries_to_TOM('awoldag', '../../../password.txt', objectids = ids, priorities = priorities, requester = 'resspect')\n",
    "    # print(ids, priorities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aabfe56-4141-4460-9fcb-ccb1bdec13b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_one_night(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea30c097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_td_loop(day):\n",
    "    # feature_extraction_method = 'Malanchev'\n",
    "    feature_extraction_method = 'laiss_resspect_classifier.elasticc2_laiss_feature_extractor.Elasticc2LaissFeatureExtractor'\n",
    "    # classifier = 'RandomForest'\n",
    "    classifier = 'laiss_resspect_classifier.laiss_classifier.LaissRandomForest'\n",
    "    \n",
    "    # run the loop to get queried objects and updated metrics\n",
    "    days = [day-1, day+1]                                # first and last day of the survey\n",
    "    training = None                           # if int take int number of objs\n",
    "                                                        # for initial training, 50% being Ia\n",
    "    \n",
    "    strategy = 'UncSampling'                        # learning strategy\n",
    "    batch = 5                                       # if int, ignore cost per observation,\n",
    "                                                         # if None find optimal batch size\n",
    "    \n",
    "    sep_files = True                               # if True, expects train, test and\n",
    "                                                        # validation samples in separate filess\n",
    "    \n",
    "    path_to_features_dir = 'TOM_days_storage/'   # folder where the files for each day are stored\n",
    "    \n",
    "    # output results for metrics\n",
    "    output_metrics_file = 'results/metrics_' + strategy + '_' + str('ini_train_set') + \\\n",
    "                           '_batch' + str(batch) +  '.csv'\n",
    "    \n",
    "    # output query sample\n",
    "    output_query_file = 'results/queried_' + strategy + '_' + str('ini_train_set') + \\\n",
    "                            '_batch' + str(batch) + '_day_'+ str(day) + '.csv'\n",
    "    \n",
    "    path_to_ini_files = {}\n",
    "    \n",
    "    # features from full light curves for initial training sample\n",
    "    path_to_ini_files['train'] = 'TOM_days_storage/TOM_training_features'\n",
    "    path_to_ini_files['test'] = 'TOM_days_storage/TOM_testing_features'\n",
    "    path_to_ini_files['validation'] = 'TOM_days_storage/TOM_validation_features'\n",
    "    \n",
    "    survey='ZTF'\n",
    "    \n",
    "\n",
    "    n_estimators = 1000                             # number of trees in the forest\n",
    "    \n",
    "\n",
    "    screen = False                                  # if True will print many things for debuging\n",
    "    fname_pattern = ['TOM_hot_features_day_', '.csv']                # pattern on filename where different days\n",
    "                                                        # are stored\n",
    "    \n",
    "    queryable= False                                 # if True, check brightness before considering\n",
    "                                                        # an object queryable    \n",
    "    \n",
    "    # run time domain loop\n",
    "    time_domain_loop(TimeDomainConfiguration(days=days, output_metrics_file=output_metrics_file,\n",
    "                     output_queried_file=output_query_file,\n",
    "                     path_to_ini_files=path_to_ini_files,\n",
    "                     path_to_features_dir=path_to_features_dir,\n",
    "                     strategy=strategy, fname_pattern=fname_pattern, batch=batch,\n",
    "                     classifier=classifier,\n",
    "                     sep_files=sep_files,\n",
    "                     survey=survey, queryable=queryable,\n",
    "                     feature_extraction_method=feature_extraction_method),\n",
    "                     screen=screen, n_estimators=n_estimators,\n",
    "                     )\n",
    "\n",
    "    #ðŸ”² do we want higher entropy in our returned objects?\n",
    "    # Read in RESSPECT requests to input to TOM format\n",
    "    ids = list(pd.read_csv(output_query_file)['id'])\n",
    "    ids = [int(id) for id in ids]\n",
    "    num = int(len(ids)/5)\n",
    "    mod = len(ids)%5\n",
    "    num_list = [num]*5\n",
    "    mod_list = []\n",
    "    for i in range(mod):\n",
    "        mod_list.append(1)\n",
    "    rem = 5-len(mod_list)\n",
    "    mod_list = mod_list+[0]*rem\n",
    "    num_list=list(np.asarray(num_list)+mod_list)\n",
    "    priorities = []\n",
    "    priorities.append([1]*num_list[0]+[2]*num_list[1]+[3]*num_list[2]+[4]*num_list[3]+[5]*num_list[4])    \n",
    "    priorities = priorities[0]\n",
    "    \n",
    "    # send these queried objects to the TOM\n",
    "    # submit_queries_to_TOM('awoldag', '../../../password.txt', objectids = ids, priorities = priorities, requester = 'resspect')\n",
    "    print(ids, priorities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "012b8082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:resspect.database:No object identification found in metadata - using first column as object identification!\n",
      "  0% (0 of 1) |                          | Elapsed Time: 0:00:00 ETA:  --:--:--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0\n",
      "Unnamed: 0\n",
      "Unnamed: 0\n",
      "Unnamed: 0\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_td_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 50\u001b[0m, in \u001b[0;36mrun_td_loop\u001b[0;34m(day)\u001b[0m\n\u001b[1;32m     46\u001b[0m queryable\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m                                 \u001b[38;5;66;03m# if True, check brightness before considering\u001b[39;00m\n\u001b[1;32m     47\u001b[0m                                                     \u001b[38;5;66;03m# an object queryable    \u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# run time domain loop\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m \u001b[43mtime_domain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTimeDomainConfiguration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdays\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_metrics_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_metrics_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m                 \u001b[49m\u001b[43moutput_queried_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_query_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mpath_to_ini_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_to_ini_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mpath_to_features_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_to_features_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname_pattern\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfname_pattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m                 \u001b[49m\u001b[43msep_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m                 \u001b[49m\u001b[43msurvey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msurvey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueryable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqueryable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mfeature_extraction_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_extraction_method\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mscreen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscreen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_estimators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m                 \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m#ðŸ”² do we want higher entropy in our returned objects?\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Read in RESSPECT requests to input to TOM format\u001b[39;00m\n\u001b[1;32m     64\u001b[0m ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(pd\u001b[38;5;241m.\u001b[39mread_csv(output_query_file)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/code/RESSPECT/src/resspect/time_domain_loop.py:847\u001b[0m, in \u001b[0;36mtime_domain_loop\u001b[0;34m(config, **kwargs)\u001b[0m\n\u001b[1;32m    841\u001b[0m light_curve_data, canonical_data \u001b[38;5;241m=\u001b[39m _update_canonical_ids(\n\u001b[1;32m    842\u001b[0m     light_curve_data, config\u001b[38;5;241m.\u001b[39mpath_to_canonical, config\u001b[38;5;241m.\u001b[39mcanonical\n\u001b[1;32m    843\u001b[0m )\n\u001b[1;32m    844\u001b[0m light_curve_data \u001b[38;5;241m=\u001b[39m _update_initial_train_meta_data_header(\n\u001b[1;32m    845\u001b[0m     first_loop_data, light_curve_data\n\u001b[1;32m    846\u001b[0m )\n\u001b[0;32m--> 847\u001b[0m \u001b[43mrun_time_domain_active_learning_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlight_curve_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[43mid_key_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlight_curve_train_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcanonical_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/RESSPECT/src/resspect/time_domain_loop.py:769\u001b[0m, in \u001b[0;36mrun_time_domain_active_learning_loop\u001b[0;34m(light_curve_data, id_key_name, light_curve_train_ids, canonical_data, config, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m progressbar\u001b[38;5;241m.\u001b[39mprogressbar(\n\u001b[1;32m    767\u001b[0m         \u001b[38;5;28mrange\u001b[39m(learning_days[\u001b[38;5;241m0\u001b[39m], learning_days[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m    768\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m light_curve_data\u001b[38;5;241m.\u001b[39mpool_features\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 769\u001b[0m         light_curve_data \u001b[38;5;241m=\u001b[39m \u001b[43m_run_classification_and_evaluation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlight_curve_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclf_bootstrap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    772\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m light_curve_data\u001b[38;5;241m.\u001b[39mqueryable_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    773\u001b[0m             object_indices \u001b[38;5;241m=\u001b[39m _get_indices_of_objects_to_be_queried(\n\u001b[1;32m    774\u001b[0m                 light_curve_data,\n\u001b[1;32m    775\u001b[0m                 budgets_dict[epoch],\n\u001b[1;32m    776\u001b[0m                 config,\n\u001b[1;32m    777\u001b[0m             )\n",
      "File \u001b[0;32m~/code/RESSPECT/src/resspect/time_domain_loop.py:287\u001b[0m, in \u001b[0;36m_run_classification_and_evaluation\u001b[0;34m(database_class, classifier, is_classifier_bootstrap, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    286\u001b[0m     database_class\u001b[38;5;241m.\u001b[39mclassify(method\u001b[38;5;241m=\u001b[39mclassifier, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 287\u001b[0m \u001b[43mdatabase_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_classification\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m database_class\n",
      "File \u001b[0;32m~/code/RESSPECT/src/resspect/database.py:1049\u001b[0m, in \u001b[0;36mDataBase.evaluate_classification\u001b[0;34m(self, metric_label, screen)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate results from classification.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \n\u001b[1;32m   1036\u001b[0m \u001b[38;5;124;03mPopulate properties: metric_list_names and metrics_list_values.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;124;03m    Default is False.\u001b[39;00m\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metric_label \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msnpcc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   1048\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics_list_names, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics_list_values \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m-> 1049\u001b[0m         \u001b[43mget_snpcc_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_class\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m                         \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1052\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOnly snpcc metric is implemented!\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1053\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Feel free to add other options.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/code/RESSPECT/src/resspect/metrics.py:152\u001b[0m, in \u001b[0;36mget_snpcc_metric\u001b[0;34m(label_pred, label_true, ia_flag, wpenalty)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_snpcc_metric\u001b[39m(label_pred: \u001b[38;5;28mlist\u001b[39m, label_true: \u001b[38;5;28mlist\u001b[39m, ia_flag\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    127\u001b[0m                      wpenalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    Calculate the metric parameters used in the SNPCC.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    149\u001b[0m \n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m     calc_eff \u001b[38;5;241m=\u001b[39m \u001b[43mefficiency\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel_pred\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mlabel_true\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mia_flag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mia_flag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m     calc_purity \u001b[38;5;241m=\u001b[39m purity(label_pred\u001b[38;5;241m=\u001b[39mlabel_pred,\n\u001b[1;32m    155\u001b[0m                          label_true\u001b[38;5;241m=\u001b[39mlabel_true, ia_flag\u001b[38;5;241m=\u001b[39mia_flag)\n\u001b[1;32m    156\u001b[0m     calc_fom \u001b[38;5;241m=\u001b[39m fom(label_pred\u001b[38;5;241m=\u001b[39mlabel_pred,\n\u001b[1;32m    157\u001b[0m                    label_true\u001b[38;5;241m=\u001b[39mlabel_true, ia_flag\u001b[38;5;241m=\u001b[39mia_flag, penalty\u001b[38;5;241m=\u001b[39mwpenalty)\n",
      "File \u001b[0;32m~/code/RESSPECT/src/resspect/metrics.py:41\u001b[0m, in \u001b[0;36mefficiency\u001b[0;34m(label_pred, label_true, ia_flag)\u001b[0m\n\u001b[1;32m     38\u001b[0m cc_ia \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([label_pred[i] \u001b[38;5;241m==\u001b[39m label_true[i] \u001b[38;5;129;01mand\u001b[39;00m label_true[i] \u001b[38;5;241m==\u001b[39m ia_flag \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(label_pred))])\n\u001b[1;32m     39\u001b[0m tot_ia \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([label_true[i] \u001b[38;5;241m==\u001b[39m ia_flag \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(label_true))])\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcc_ia\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtot_ia\u001b[49m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "run_td_loop(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d8cba2-e04d-44e8-84f1-0905365fae94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pull classified obj and add them to the training set\n",
    "def get_classified(username, passwordfile=None, password=None, since = None):\n",
    "    tom = TomClient(url = \"https://desc-tom-2.lbl.gov\", username = username, password = password, \n",
    "                    passwordfile = passwordfile)\n",
    "    dic = {}\n",
    "    if since is not None:\n",
    "        dic['since'] = since\n",
    "\n",
    "    res = tom.post( 'elasticc2/getknownspectruminfo', json=dic )\n",
    "\n",
    "    assert res.status_code == 200\n",
    "    assert res.json()['status'] == \"ok\"\n",
    "    reqs = res.json()\n",
    "    return reqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88f51ec-a375-4563-aaad-77582ea984ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "classed_obj = get_classified('amandaw8', passwordfile='/Users/arw/secrets/TOM2', since = '11/22/2024 19:20:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04029ea7-337b-4efe-9f98-286487b35989",
   "metadata": {},
   "outputs": [],
   "source": [
    "objectids = []\n",
    "classes = []\n",
    "for obj in classed_obj['spectra']:\n",
    "    objectids.append(obj['objectid'])\n",
    "    if obj['classid'] == 2222:\n",
    "        classes.append('Ia')\n",
    "    else:\n",
    "        classes.append('other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8af7dc4-d30b-4c5d-b2ec-4770bb2f4581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_object_phot(username, passwordfile=None, password=None, obj_ids=[]):\n",
    "    tom = TomClient(url = \"https://desc-tom-2.lbl.gov\", username = username, password = password, \n",
    "                    passwordfile = passwordfile)\n",
    "    dic = {'obj_ids': obj_ids}\n",
    "\n",
    "    res = tom.post( 'elasticc2/getobjphot', json = dic)\n",
    "\n",
    "    assert res.status_code == 200\n",
    "    assert res.json()['status'] == \"ok\"\n",
    "    reqs = res.json()\n",
    "    return reqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dcb4e8-ee8c-4ef4-ae6c-9d97ac6bd963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_training_set(objectids, classes):\n",
    "    #need to fetch the current features of the labeled objs (probably from the TOM, get the features and format correctly) \n",
    "    #! figure out how to get a specific objid photometry\n",
    "    #IN THE FUTURE get this from our mongodb, in the mean time though only god knows which files will contain which objects\n",
    "\n",
    "\n",
    "    # call something like elasticc2/getobjphot\n",
    "    data_dic = get_object_phot(amandaw8, passwordfile = '/Users/arw/secrets/tom2', obj_ids = objectids)\n",
    "\n",
    "    # put the ^ dictionary into the right format to get features\n",
    "    data_dic=data_dic['diaobject'] \n",
    "    \n",
    "    # then do something like fit_TOM to get the features from the object photometry\n",
    "    outdir = 'TOM_train_features_storage'\n",
    "    file_name = outdir+'/TOM_train_features_day_'+str(day)+'.csv'\n",
    "        \n",
    "    fit_TOM(data_dic, output_features_file = file_name, feature_extractor = 'Malanchev')\n",
    "    data = pd.read_csv(file_name, index_col=False)\n",
    "    data['orig_sample'] = 'train'\n",
    "    \n",
    "    data.to_csv(file_name,index=False)\n",
    "\n",
    "    # then do something like read this file in and concatenate it with the current training set\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    #REMOVE classified objects from the pool set each night. Just double check that \n",
    "    #all SN in hot transients DO NOT have same object ids as those in the training set\n",
    "    #CHECK WITH ROB - can we make it so that gethotsne removed classified obj\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2932e2c0-7dc4-4344-bc51-1629487d4dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add this classification and features to training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398a6b2b-ddb7-48ee-ad4d-a6b238167c56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d46352-1c42-4b44-a0e1-1cf0d478b3e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a6fc9a-e153-4e36-b2b8-d0cb486ec4a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "laiss_resspect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
